{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cardazuluaga/DLCourse/blob/main/Ej3FuncionesActivacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD8CCHb10qy1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "kFA_kM3z1sqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar las etiquetas en formato one-hot para representar las clases de manera binaria clase 1 -> [1 0 0], clase 2-> [0 1 0] ....\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_qNhIPNt1tKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir funciones de activación\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x >= 0, x, alpha * x)"
      ],
      "metadata": {
        "id": "NctFSL-E1tiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para entrenar y evaluar una red neuronal con una función de activación dada\n",
        "def train_and_evaluate(activation_function):\n",
        "    np.random.seed(42)  # Para reproducibilidad\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_size = 5\n",
        "    output_size = y_train.shape[1]\n",
        "    learning_rate = 0.001\n",
        "    epochs = 1000\n",
        "\n",
        "    # Inicialización de pesos y sesgos\n",
        "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "    bias_hidden = np.zeros((1, hidden_size))\n",
        "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "    bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Feedforward\n",
        "        hidden_output = activation_function(np.dot(X_train, weights_input_hidden) + bias_hidden)\n",
        "        predicted_output = softmax(np.dot(hidden_output, weights_hidden_output) + bias_output)\n",
        "\n",
        "        # Backpropagation\n",
        "        error = y_train - predicted_output\n",
        "        delta_output = error * (predicted_output * (1 - predicted_output))\n",
        "        delta_hidden = delta_output.dot(weights_hidden_output.T) * (hidden_output * (1 - hidden_output))\n",
        "\n",
        "        # Actualizar pesos y sesgos\n",
        "        weights_hidden_output += hidden_output.T.dot(delta_output) * learning_rate\n",
        "        bias_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
        "        weights_input_hidden += X_train.T.dot(delta_hidden) * learning_rate\n",
        "        bias_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    # Evaluación en datos de prueba\n",
        "    hidden_output_test = activation_function(np.dot(X_test, weights_input_hidden) + bias_hidden)\n",
        "    predicted_output_test = softmax(np.dot(hidden_output_test, weights_hidden_output) + bias_output)\n",
        "\n",
        "    accuracy = np.mean(np.argmax(predicted_output_test, axis=1) == np.argmax(y_test, axis=1))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "pIo2huGG1t7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos, El término \"accuracy\" se refiere a la precisión de un modelo de clasificación, es decir, la proporción de ejemplos clasificados correctamente en relación con el total de ejemplos. En otras palabras, la precisión mide qué tan cerca está el modelo de clasificación de predecir correctamente las etiquetas de las muestras.\n",
        "\n",
        "Por ejemplo, si tienes 100 muestras en total y tu modelo clasifica correctamente 85 de ellas, la precisión sería 85100=0.8510085​=0.85, lo que equivale al 85%."
      ],
      "metadata": {
        "id": "fvzUKrhe4TsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de activación Softmax\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "dDoUEGZU2C6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar y comparar las funciones de activación\n",
        "activation_functions = [sigmoid, tanh, relu, leaky_relu]\n",
        "for activation_function in activation_functions:\n",
        "    accuracy = train_and_evaluate(activation_function)\n",
        "    activation_name = activation_function.__name__\n",
        "    print(f'Activación: {activation_name}, Precisión: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "NipilTjQ2N-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}